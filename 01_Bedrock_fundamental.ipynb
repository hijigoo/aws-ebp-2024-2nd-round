{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a23d2a-307f-44be-ae61-b5d27cf3e4bf",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0230ad-e3af-42ac-97f0-e509e23fc063",
   "metadata": {},
   "source": [
    "> 이 노트북은, SageMaker Notebook <i><b>conda_python3</b></i> 커널에서 테스트 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce1a91-782c-4651-af7e-8cfa1cebd287",
   "metadata": {},
   "source": [
    "## 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb933d-494d-490e-8bb7-33c94ff00073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 설치합니다.\n",
    "# !pip install -r ./requirements.txt\n",
    "%pip install boto3\n",
    "%pip install botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577d7c6-6e93-4b05-99a3-e4d44623779b",
   "metadata": {},
   "source": [
    "## 1. boto3를 활용한 Bedrock Claude 3 호출 예시 (not LangChain)\n",
    "\n",
    "#### 1-1. Bedrock 에서 사용 가능한 리스트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a37b3-87e8-486b-bab2-29c581be4d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "\n",
    "# Bedrock Client 생성\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "\n",
    "# 사용가능한 모델 확인하기\n",
    "model_list = bedrock.list_foundation_models()\n",
    "ondemand_models = []\n",
    "for model_info in model_list[\"modelSummaries\"]:\n",
    "    if model_info['inferenceTypesSupported'] == ['ON_DEMAND']:\n",
    "        ondemand_models.append((model_info[\"modelName\"], model_info[\"modelId\"]))\n",
    "\n",
    "pprint(ondemand_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e1091-88fe-42ff-bef2-77e7e71c5bca",
   "metadata": {},
   "source": [
    "#### 1-2. Claude 3 모델 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fa915-701d-45fd-835c-f3fed2d61877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bedrock runtime 생성\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8070fa-1374-4a76-a82d-76f9838f3045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 사용할 모델 아이디 작성\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# Request body 작성\n",
    "query = \"\"\"당신은 인공지능 AI 보험 서비스입니다. 생명과 손해 보험의 차이에 대해 설명해 주세요.\"\"\"\n",
    "request_data = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1.0,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "request_body = json.dumps(request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e36802-8a01-4a83-bf12-4883cee9fdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 호출\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=request_body,\n",
    "    modelId=modelId,\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "response_body = json.loads(response['body'].read())\n",
    "answer = response_body['content'][0]['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2535019",
   "metadata": {},
   "source": [
    "#### 1-3. Claude 3 모델 스트리밍으로 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf73bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock runtime 생성\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모델 아이디 작성\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# Request body 작성\n",
    "query = \"\"\"당신은 인공지능 AI 보험 서비스입니다. 생명과 손해 보험의 차이에 대해 설명해 주세요.\"\"\"\n",
    "request_data = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1.0,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "request_body = json.dumps(request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50674ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming 으로 모델 호출\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    body=request_body,\n",
    "    modelId=modelId,\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "stream = response.get('body')\n",
    "answer = \"\"\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get('chunk')\n",
    "        if chunk:\n",
    "            chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "            if \"delta\" in chunk_obj:                    \n",
    "                delta = chunk_obj['delta']\n",
    "                if \"text\" in delta:\n",
    "                    text=delta['text'] \n",
    "                    print(text, end=\"\")\n",
    "                    answer+=str(text) \n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd0420-5119-439b-9068-ba657b08e4c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a20dd8-ab80-409e-8a8a-edc42d02fd86",
   "metadata": {},
   "source": [
    "## 2. LangChain을 활용한 Bedrock 사용 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0609f-4376-4d18-9190-0fb0ac71467d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "%pip install langchain==0.2.1\n",
    "%pip install langchain_aws==0.1.6\n",
    "%pip install langchain_community==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3715fd-4f98-4fd1-b1e2-7545e761a394",
   "metadata": {},
   "source": [
    "#### 2-1. LangChain의 ChatBedrock을 활용한 간단한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04aee2-7b0f-4b65-87a7-8da55431bda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "# langchain.debug=True\n",
    "\n",
    "# ChatBedrock llm 생성\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.5,\n",
    "        \"top_k\": 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 입력: Option 1\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"안녕? 만나서 반가워\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 입력: Option 2\n",
    "# messages = [\n",
    "#     (\"human\", \"안녕? 만나서 반가워\"),\n",
    "# ]\n",
    "\n",
    "# 입력: Option 3\n",
    "# message = \"안녕? 만나서 반가워\"\n",
    "\n",
    "# 일반 방식으로 모델 호출\n",
    "response = llm.invoke(messages)\n",
    "answer = response.content\n",
    "print(f\"응답:\")\n",
    "print(answer)\n",
    "print()\n",
    "\n",
    "\n",
    "# 스트리밍 방식으로 모델 호출\n",
    "print(\"스트리밍 응답:\")\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()\n",
    "\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1c59f-2c5f-4bd8-a797-1fd8a6a2909b",
   "metadata": {},
   "source": [
    "#### 2-2. LangChain의 BedrockChat을 활용한 멀티 턴(대화 히스토리 유지) 구현한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd328b2-d24f-46b2-b73b-90e71346e9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.5,\n",
    "    \"top_k\": 0\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "        region_name=\"us-west-2\",\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs\n",
    "    )\n",
    "\n",
    "memroy_window = 10\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(\n",
    "        k=memroy_window,\n",
    "        ai_prefix='AI',\n",
    "        human_prefix='Human'\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 일반 방식으로 모델 호출\n",
    "while True:\n",
    "    input_text = input(\"Human: \")\n",
    "    answer = conversation_chain.predict(input=input_text)\n",
    "    print(\"AI:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 방식으로 모델 호출\n",
    "class StreamingHandler(BaseCallbackHandler, ABC):\n",
    "    \"\"\"\n",
    "    Callback handler to stream the generated text to Streamlit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.text = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Append the new token to the text and update the Streamlit container.\n",
    "        \"\"\"\n",
    "        self.text += token\n",
    "        print(token, end =\"\")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "        region_name=\"us-west-2\",\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingHandler()]\n",
    "    )\n",
    "\n",
    "memroy_window = 10\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(\n",
    "        k=memroy_window,\n",
    "        ai_prefix='AI',\n",
    "        human_prefix='Human'\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "while True:\n",
    "    input_text = input(\"Human: \")\n",
    "    answer = conversation_chain.predict(input=input_text)\n",
    "    print(\"AI:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc02a25-4720-449f-9c14-6ceebacfd178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
